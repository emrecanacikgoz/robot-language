seed: 42

task: "mlp" # [mlp, rnn, gpt]

trainer:
  accelerator: "auto"          # ["cpu", "gpu", "tpu", "ipu", "auto"]
  devices: "auto"
  max_epochs: 100
  max_steps: -1                # Disabled when -1
  precision: 32                # [64, 32, "bf16", 16]
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 5
  auto_scale_batch_size: False
  auto_lr_find: False
  strategy: null               # [None, "ddp", "ddp_spawn"]
  resume_from_checkpoint: null # [None, "some/path/to/my_checkpoint.ckpt"]
  enable_progress_bar: True
  logger: "wandb"              # [False, "wandb", "tensorboard"]
  wandb_project: "dummy"
  wandb_name: "dummy-run"

training:
  batch_size: 32
  lr: "1e-4"
  lr_factor: 0.5
  lr_patience: 5
  lr_cooldown: 0
  min_lr: "1e-6"
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01
  gradient_clip_val: 1.0

data:
#  val_data_file_tsv: "/kuacc/users/eacikgoz17/robot-language/data/ABCD-validation.tsv"
#  train_data_file_tsv: "/kuacc/users/eacikgoz17/robot-language/data/ABCD-training.tsv"

  train_path: "data/ABC_training.pkl"
  val_path: "data/D_validation.pkl"
  keys: [
         "actions",                         # 7-dimensional
         "rel_actions",                     # 7-dimensional
         #"robot_obs",                      # 15-dimensional
         "robot_obs_tcp_position",          # 3-dimensional
         "robot_obs_tcp_orientation",       # 3-dimensional
         "robot_obs_gripper_opening_width", # 1-dimensional
         "robot_obs_arm_joint_states",      # 7-dimensional
         "robot_obs_gripper_action",        # 1-dimensional
         "scene_obs"                        # 24-dimensional
        ]
  max_length: 64
  window: 64
  num_workers: 2
  pin_memory: False
  shuffle_train: True
  shuffle_val: False

model_mlp:
  input_dim: 6305
  hidden_dim: 128
  output_dim: 34
  dropout: 0.1
  bias: True
  activation: "relu" # ["gelu", "relu"]

model_rnn:
  rnn: "lstm"         #["rnn", "lstm", "gru"]
  input_size: 53
  hidden_size: 128
  num_layers: 1
  bias: True
  dropout: 0.3
  output_dim: 34
  final_output_rnn: "mean" # ["last_time_step", "mean"]

model_gpt:
  block_size: 1024 # max context length, also used in max positional embeddings
  vocab_size: Null # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  n_layer: 4
  n_head: 4
  n_embd: 52
  dropout: 0.0
  bias: True
  activation: "gelu" # ["gelu", "relu"]
  feedforward: "fc" # ["fc", "cnn"]
  loss: "mse" # ["softmax", "mse"]

# hydra-specific
hydra:
  run:
    dir: .
