seed: 42
output: output.txt

trainer:
  accelerator: "cpu"           # ["cpu", "gpu", "tpu", "ipu", "auto"]
  devices: 1
  max_epochs: 10
  max_steps: -1                # Disabled when -1
  gradient_clip_val: 1.0
  precision: 16                # [64, 32, "bf16", 16]
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  auto_scale_batch_size: False 
  auto_lr_find: False
  strategy: null               # [None, "ddp", "ddp_spawn"]
  resume_from_checkpoint: null # [None, "some/path/to/my_checkpoint.ckpt"]
  logger: False                # [False, "wandb", "tensorboard"]

# data module
data:
  train_data_dir: "/Users/emrecanacikgoz/Desktop/rl-project/calvin_debug_dataset/training"
  val_data_dir: "/Users/emrecanacikgoz/Desktop/rl-project/calvin_debug_dataset/training"
  keys: ["actions", "rel_actions", "robot_obs"] # Can be "actions", "rel_actions", "robot_obs"
  batch_size: 3
  num_workers: 0
  pin_memory: False

model:
  block_size: 1024 # also used in max positional embeddings
  vocab_size: null # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.0
  bias: True
  activation: "gelu" # ["gelu", "relu"]
  mlp: "fc" # ["fc", "cnn"]

# hydra-specific
hydra:
  run:
    dir: .
