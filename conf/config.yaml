seed: 42
output: output.txt

trainer:
  accelerator: "cpu"
  devices: 1
  max_epochs: 10
  precision: 16
  gradient_clip_val: 1.0
  resume_from_checkpoint: null
  accumulate_grad_batches: 1

# data module
data:
  train_data_dir: "/Users/emrecanacikgoz/Desktop/rl-project/calvin_debug_dataset/training"
  val_data_dir: "/Users/emrecanacikgoz/Desktop/rl-project/calvin_debug_dataset/training"
  keys: ["actions", "rel_actions", "robot_obs"] # Can be "actions", "rel_actions", "robot_obs"
  batch_size: 3
  num_workers: 0
  pin_memory: False

model:
  block_size: 1024
  vocab_size: null # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.0
  bias: True

# hydra-specific
hydra:
  run:
    dir: .
