seed: 42

trainer:
  accelerator: "auto"          # ["cpu", "gpu", "tpu", "ipu", "auto"]
  devices: "auto"
  max_epochs: 100
  max_steps: -1                # Disabled when -1
  gradient_clip_val: 1.0
  precision: 32                # [64, 32, "bf16", 16]
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 5
  auto_scale_batch_size: False
  auto_lr_find: False
  strategy: null               # [None, "ddp", "ddp_spawn"]
  resume_from_checkpoint: null # [None, "some/path/to/my_checkpoint.ckpt"]
  enable_progress_bar: True
  logger: "wandb"              # [False, "wandb", "tensorboard"]
  wandb_project: "deneme"
  wandb_name: "deneme-1"


# data module
data:
  task: "mlp" # [gpt, mlp]
#  train_data_file_tsv: "/kuacc/users/eacikgoz17/robot-language/data/ABCD-training.tsv"
#  val_data_file_tsv: "/kuacc/users/eacikgoz17/robot-language/data/ABCD-validation.tsv"
#  train_data_dir_npy: "/datasets/calvin/ABCD/training"
#  val_data_dir_npy: "/datasets/calvin/ABCD/validation"

  train_data_file_tsv: "/Users/emrecanacikgoz/Desktop/rl-project/data/D_tsv/debug-training.tsv"
  val_data_file_tsv: "/Users/emrecanacikgoz/Desktop/rl-project/data/D_tsv/debug-validation.tsv"
  train_data_dir_npy: "/Users/emrecanacikgoz/Desktop/rl-project/data/calvin_debug_dataset/training"
  val_data_dir_npy: "/Users/emrecanacikgoz/Desktop/rl-project/data/calvin_debug_dataset/validation"

  keys: [
         #"actions",                         # 7-dimensional
         #"rel_actions",                     # 7-dimensional
         #"robot_obs",                      # 15-dimensional
         "robot_obs_tcp_position",          # 3-dimensional
         "robot_obs_tcp_orientation",       # 3-dimensional
         "robot_obs_gripper_opening_width", # 1-dimensional
         "robot_obs_arm_joint_states",      # 7-dimensional
         "robot_obs_gripper_action",        # 1-dimensional
         "scene_obs"                        # 24-dimensional
        ]
  max_length: 64
  batch_size: 2
  lr: "1e-4"
  lr_factor: 0.5
  lr_patience: 5
  lr_cooldown: 0
  min_lr: "1e-6"
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01
  num_workers: 4
  pin_memory: False
  shuffle_train: True
  shuffle_val: False

model_gpt:
  block_size: 1024 # max context length, also used in max positional embeddings
  vocab_size: Null # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  n_layer: 4
  n_head: 4
  n_embd: 52
  dropout: 0.0
  bias: True
  activation: "gelu" # ["gelu", "relu"]
  feedforward: "fc" # ["fc", "cnn"]
  loss: "mse" # ["softmax", "mse"]

model_mlp:
  input_dim: 39
  hidden_dim: 128
  output_dim: 34
  dropout: 0.1
  bias: True
  activation: "relu" # ["gelu", "relu"]

model_rnn:
  rnn: "rnn"         #["rnn", "lstm", "gru"]
  input_size: 39
  hidden_size: 128
  num_layers: 1
  bias: True
  dropout: 0.1
  output_dim: 34


# hydra-specific
hydra:
  run:
    dir: .
